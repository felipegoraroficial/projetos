# Nintendo SWITCH - GOOGLE CLOUD PIPELINE

I'm excited to share my recent project with you! Recently, I focused on the world of Google Cloud and DBT and created a data analysis system focused on the video game console market, with a special emphasis on the Nintendo Switch.

The central element of the project is the integration of the Google Cloud Storage's three-layer framework - raw, silver, and gold - with the powerful analysis capabilities of DBT (Data Build Tool) on Google BigQuery. This approach offers an effective and scalable organization for the collected data, establishing a solid foundation for advanced analytics.

Using the Beautiful Soup library in Python, I automated the data collection from three major online retailers. These data are then stored in the raw layer of the bucket, maintaining their original integrity.

Next, I initiated the process of data transformation and preparation. I extracted relevant information from the raw layer's HTML files and organized them into lists, subsequently stored in JSON format in the silver layer. This procedure is crucial for further analysis.

In the final phase of the process, the data is transformed into dataframes, where various cleaning and manipulation operations are applied. The resulting dataframes are then converted into Parquet files and stored in the gold layer of the bucket, and finally, the data is loaded into the Google BigQuery table.

Orchestration work is performed through Apache Airflow, leveraging the robust infrastructure of Google Cloud Composer. This ensures reliable and efficient execution of tasks, regardless of scale.

With the data available in Google BigQuery, I used DBT to unify and refine the tables, ensuring a consistent and accurate view of the data. I implemented filters to ensure only relevant information about consoles is included in our analyses.

Finally, using Looker Data Studio, I created a simple interactive dashboard directly connected to the view generated by DBT. This provides an engaging and accessible analytical experience for the entire team.

# Technologies Used

- Google Cloud Storage: Used to store data in different layers (raw, silver, and gold).
- Google BigQuery: Database used for data analysis.
- DBT (Data Build Tool): Tool used for data transformation and preparation.
- Apache Airflow: Orchestration tool for reliable and efficient task execution.
- Google Cloud Composer: Used to orchestrate Apache Airflow.
- Python: Used for data collection automation with the Beautiful Soup library.
- Looker Data Studio: Used to create interactive dashboards connected directly to DBT.

# Key Features

Data Collection: Utilization of the Beautiful Soup library in Python to automate data collection from major online retailers. Data is stored in the raw layer of Google Cloud Storage.

Data Transformation and Preparation: The collected data is transformed and prepared for analysis. Relevant information is extracted from HTML files and organized into lists, subsequently stored in JSON format in the silver layer of Google Cloud Storage. Then, the data is transformed into dataframes, where various cleaning and manipulation operations are applied. The resulting dataframes are converted into Parquet files and stored in the gold layer of Google Cloud Storage.

Orchestration: Utilization of Apache Airflow, leveraging Google Cloud Composer's robust infrastructure, for process orchestration. This ensures reliable and efficient execution of tasks, regardless of scale.

Data Analysis with DBT: Utilization of DBT to unify and refine tables, ensuring a consistent and accurate view of the data. Implementation of filters to include only relevant information about consoles in our analyses.

Interactive Dashboard: Creation of a simple interactive dashboard using Looker Data Studio, directly connected to the view generated by DBT. This provides an engaging and accessible analytical experience for the entire team.

# How to Contribute

Contributions are welcome! Feel free to open issues to report bugs, suggest enhancements, or propose new features.
If you wish to contribute directly, please fork the repository, make the changes, and open a pull request.



