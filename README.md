# Projetos
This repository was created to showcase a collection of scripts demonstrating my expertise in building data pipelines. Here, you'll find functions ranging from data extraction, whether from databases, APIs, or utilizing Web Scrapy, to data processing, modeling, and storage in Datalakes or databases.

The scripts were developed in Python and orchestrated by Apache Airflow. All storage is done in databases and Datalakes within the Azure environment.

Key aspects covered:

Data extraction from various sources such as databases, APIs, and Web Scrapy.
Data processing and modeling for preparation purposes.
Secure and efficient storage in Datalakes and databases within the Azure ecosystem.
Feel free to explore the code and scripts to understand the approach used in building these data pipelines.

How to Use

You can clone this repository to your computer or download the files individually. Each script has a comment explaining what it does, what the parameters are, the dependencies, and the expected result. You can run the scripts in your terminal or in an Integrated Development Environment (IDE) of your preference.

How to Contribute

If you'd like to contribute to this repository, feel free to send a pull request or open an issue. I'll be happy to receive feedback, suggestions, corrections, or new script functions. Please follow good coding and documentation practices and respect GitHub's code of conduct.
