# Projetos
Este repositório foi criado para apresentar uma coleção de scripts que demonstram minha expertise na construção de pipelines de dados. Aqui, você encontrará funções abrangendo desde a extração de dados, seja de bancos de dados, APIs ou utilizando Web Scrapy, até o tratamento, modelagem e armazenamento dos dados em Datalakes ou bancos de dados.

Os scripts foram desenvolvidos em Python e são orquestrados pelo Apache Airflow. Todo o armazenamento é realizado nos bancos de dados e Datalakes do ambiente Azure.

Principais aspectos abordados:

Extração de dados de diversas fontes, como bancos de dados, APIs e Web Scrapy.
Processamento e modelagem dos dados para preparação.
Armazenamento seguro e eficiente em Datalakes e bancos de dados do ecossistema Azure.
Sinta-se à vontade para explorar o código e os scripts para entender a abordagem utilizada na construção desses pipelines de dados.

#Como usar
Você pode clonar este repositório disponiveis para o seu computador ou baixar os arquivos individualmente. Cada script tem um comentário explicando o que ele faz, quais são os parâmetros, as dependências e o resultado esperado. Você pode executar os scripts no seu terminal ou em um ambiente de desenvolvimento integrado (IDE) de sua preferência.

#Como contribuir
Se você quiser contribuir com este repositório, sinta-se à vontade para enviar uma solicitação de pull ou abrir uma issue. Eu ficarei feliz em receber feedback, sugestões, correções ou novas funções de script. Por favor, siga as boas práticas de codificação e documentação, e respeite o código de conduta do GitHub.
